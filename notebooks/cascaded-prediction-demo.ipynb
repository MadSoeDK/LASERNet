{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cascaded Prediction Demo\n",
    "\n",
    "This notebook demonstrates cascaded predictions where:\n",
    "1. The temperature model predicts the next temperature field\n",
    "2. The predicted temperature is used as input to the microstructure model to predict the next microstructure\n",
    "\n",
    "We evaluate on all slices in timestep 18 (first test timestep)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "\n",
    "sys.path.insert(0, \"../src\")\n",
    "\n",
    "from lasernet.models.deep_cnn_lstm import DeepCNN_LSTM_Large\n",
    "from lasernet.data.dataset import LaserDataset\n",
    "from lasernet.utils import compute_index, get_num_of_slices\n",
    "from lasernet.visualize import plot_temperature_prediction, plot_microstructure_prediction\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load the Trained Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model checkpoint paths\n",
    "temp_checkpoint_path = Path(\"../models/best_deepcnn_lstm_large_temperature_mseloss.ckpt\")\n",
    "micro_checkpoint_path = Path(\"../models/best_deepcnn_lstm_large_microstructure_mseloss.ckpt\")\n",
    "\n",
    "# Check paths exist\n",
    "if not temp_checkpoint_path.exists():\n",
    "    raise FileNotFoundError(f\"Temperature model not found at {temp_checkpoint_path}\")\n",
    "if not micro_checkpoint_path.exists():\n",
    "    raise FileNotFoundError(f\"Microstructure model not found at {micro_checkpoint_path}\")\n",
    "\n",
    "print(f\"Loading temperature model from {temp_checkpoint_path}\")\n",
    "print(f\"Loading microstructure model from {micro_checkpoint_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load temperature model\n",
    "temp_model = DeepCNN_LSTM_Large.load_from_checkpoint(\n",
    "    temp_checkpoint_path,\n",
    "    field_type=\"temperature\",\n",
    "    input_channels=1,\n",
    "    output_channels=1,\n",
    ")\n",
    "temp_model = temp_model.to(device)\n",
    "temp_model = temp_model.half()  # Convert to float16 for memory efficiency\n",
    "temp_model.eval()\n",
    "\n",
    "print(f\"Temperature model loaded successfully\")\n",
    "print(f\"  Parameters: {temp_model.count_parameters():,}\")\n",
    "print(f\"  dtype: {next(temp_model.parameters()).dtype}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load microstructure model\n",
    "micro_model = DeepCNN_LSTM_Large.load_from_checkpoint(\n",
    "    micro_checkpoint_path,\n",
    "    field_type=\"microstructure\",\n",
    "    input_channels=11,  # 10 microstructure + 1 temperature\n",
    "    output_channels=10,  # 10 microstructure channels\n",
    ")\n",
    "micro_model = micro_model.to(device)\n",
    "micro_model = micro_model.half()  # Convert to float16 for memory efficiency\n",
    "micro_model.eval()\n",
    "\n",
    "print(f\"Microstructure model loaded successfully\")\n",
    "print(f\"  Parameters: {micro_model.count_parameters():,}\")\n",
    "print(f\"  dtype: {next(micro_model.parameters()).dtype}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load temperature training dataset to get normalizer\n",
    "temp_train_dataset = LaserDataset(\n",
    "    data_path=Path(\"../data/processed/\"),\n",
    "    field_type=\"temperature\",\n",
    "    split=\"train\",\n",
    "    normalize=True,\n",
    "    plane=\"xz\",\n",
    "    sequence_length=3,\n",
    "    target_offset=1,\n",
    ")\n",
    "print(f\"Temperature train dataset: {len(temp_train_dataset)} samples\")\n",
    "\n",
    "# Load temperature test dataset with same normalizer\n",
    "temp_test_dataset = LaserDataset(\n",
    "    data_path=Path(\"../data/processed/\"),\n",
    "    field_type=\"temperature\",\n",
    "    split=\"test\",\n",
    "    normalize=True,\n",
    "    normalizer=temp_train_dataset.normalizer,\n",
    "    plane=\"xz\",\n",
    "    sequence_length=3,\n",
    "    target_offset=1,\n",
    ")\n",
    "print(f\"Temperature test dataset: {len(temp_test_dataset)} samples\")\n",
    "print(f\"Temperature data shape: {temp_test_dataset.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load microstructure training dataset to get normalizer\n",
    "micro_train_dataset = LaserDataset(\n",
    "    data_path=Path(\"../data/processed/\"),\n",
    "    field_type=\"microstructure\",\n",
    "    split=\"train\",\n",
    "    normalize=True,\n",
    "    plane=\"xz\",\n",
    "    sequence_length=3,\n",
    "    target_offset=1,\n",
    ")\n",
    "print(f\"Microstructure train dataset: {len(micro_train_dataset)} samples\")\n",
    "\n",
    "# Load microstructure test dataset with same normalizer\n",
    "micro_test_dataset = LaserDataset(\n",
    "    data_path=Path(\"../data/processed/\"),\n",
    "    field_type=\"microstructure\",\n",
    "    split=\"test\",\n",
    "    normalize=True,\n",
    "    normalizer=micro_train_dataset.normalizer,\n",
    "    plane=\"xz\",\n",
    "    sequence_length=3,\n",
    "    target_offset=1,\n",
    ")\n",
    "print(f\"Microstructure test dataset: {len(micro_test_dataset)} samples\")\n",
    "print(f\"Microstructure data shape: {micro_test_dataset.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Cascaded Prediction for Timestep 18\n",
    "\n",
    "Timestep 18 is the first timestep in the test split. We'll predict all 94 slices (xz plane)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "target_timestep = 18  # First test timestep\n",
    "plane = \"xz\"\n",
    "num_slices = get_num_of_slices(plane)  # 94 slices for xz plane\n",
    "\n",
    "print(f\"Target timestep: {target_timestep}\")\n",
    "print(f\"Plane: {plane}\")\n",
    "print(f\"Number of slices: {num_slices}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cascaded_prediction(\n",
    "    temp_model,\n",
    "    micro_model,\n",
    "    temp_dataset,\n",
    "    micro_dataset,\n",
    "    slice_idx,\n",
    "    device,\n",
    "):\n",
    "    \"\"\"\n",
    "    Perform cascaded prediction for a single slice.\n",
    "    \n",
    "    Pipeline:\n",
    "    1. Get temperature input sequence and predict next temperature\n",
    "    2. Get microstructure input sequence\n",
    "    3. Replace the temperature channel in the last frame with predicted temperature\n",
    "    4. Predict microstructure using the modified input\n",
    "    \n",
    "    Returns:\n",
    "        dict with predictions and ground truths\n",
    "    \"\"\"\n",
    "    # Get temperature data\n",
    "    temp_input_seq, temp_target, _, temp_mask = temp_dataset[slice_idx]\n",
    "    \n",
    "    # Get microstructure data\n",
    "    micro_input_seq, micro_target, target_temperature, micro_mask = micro_dataset[slice_idx]\n",
    "    \n",
    "    # ===== Step 1: Predict temperature =====\n",
    "    temp_input_batch = temp_input_seq.unsqueeze(0).half().to(device)  # [1, seq_len, 1, H, W]\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        temp_pred = temp_model(temp_input_batch)  # [1, 1, H, W]\n",
    "    \n",
    "    # ===== Step 2: Prepare microstructure input with predicted temperature =====\n",
    "    # The microstructure input has 11 channels: 10 microstructure + 1 temperature\n",
    "    # We need to normalize the predicted temperature to match microstructure dataset normalization\n",
    "    \n",
    "    # Denormalize predicted temperature using temperature normalizer\n",
    "    temp_pred_denorm = temp_dataset.denormalize(temp_pred.cpu().float())  # [1, 1, H, W]\n",
    "    \n",
    "    # Normalize using microstructure normalizer (temperature is the last channel, index 10)\n",
    "    if micro_dataset.normalizer is not None:\n",
    "        temp_min = micro_dataset.normalizer.channel_mins[10].item()\n",
    "        temp_max = micro_dataset.normalizer.channel_maxs[10].item()\n",
    "        temp_pred_norm_micro = (temp_pred_denorm - temp_min) / (temp_max - temp_min)\n",
    "    else:\n",
    "        temp_pred_norm_micro = temp_pred_denorm\n",
    "    \n",
    "    # Create cascaded microstructure input:\n",
    "    # Use original microstructure sequence, but replace temperature in last frame with prediction\n",
    "    micro_input_cascaded = micro_input_seq.clone()\n",
    "    micro_input_cascaded[-1, 10:11, :, :] = temp_pred_norm_micro[0]  # Replace last frame's temperature channel\n",
    "    \n",
    "    # ===== Step 3: Predict microstructure =====\n",
    "    micro_input_batch = micro_input_cascaded.unsqueeze(0).half().to(device)  # [1, seq_len, 11, H, W]\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        micro_pred_cascaded = micro_model(micro_input_batch)  # [1, 10, H, W]\n",
    "    \n",
    "    # ===== For comparison: Standard microstructure prediction (with ground truth temperature) =====\n",
    "    micro_input_batch_standard = micro_input_seq.unsqueeze(0).half().to(device)\n",
    "    with torch.no_grad():\n",
    "        micro_pred_standard = micro_model(micro_input_batch_standard)  # [1, 10, H, W]\n",
    "    \n",
    "    return {\n",
    "        # Temperature\n",
    "        \"temp_input_seq\": temp_input_seq.cpu(),\n",
    "        \"temp_target\": temp_target.cpu(),\n",
    "        \"temp_pred\": temp_pred.cpu().float(),\n",
    "        \"temp_mask\": temp_mask.cpu(),\n",
    "        # Microstructure\n",
    "        \"micro_input_seq\": micro_input_seq.cpu(),\n",
    "        \"micro_target\": micro_target.cpu(),\n",
    "        \"micro_pred_cascaded\": micro_pred_cascaded.cpu().float(),\n",
    "        \"micro_pred_standard\": micro_pred_standard.cpu().float(),\n",
    "        \"micro_mask\": micro_mask.cpu(),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run cascaded prediction for all slices in timestep 18\n",
    "# Timestep 18 corresponds to relative timestep 0 in test split (first test timestep)\n",
    "# The dataset indices for temporal_offset=0 are: slice_idx = 0 to 93\n",
    "\n",
    "results = []\n",
    "\n",
    "print(f\"Running cascaded predictions for {num_slices} slices...\")\n",
    "for slice_idx in range(num_slices):\n",
    "    result = cascaded_prediction(\n",
    "        temp_model=temp_model,\n",
    "        micro_model=micro_model,\n",
    "        temp_dataset=temp_test_dataset,\n",
    "        micro_dataset=micro_test_dataset,\n",
    "        slice_idx=slice_idx,  # slice_idx directly since temporal_offset=0 for first timestep\n",
    "        device=device,\n",
    "    )\n",
    "    results.append(result)\n",
    "    \n",
    "    if (slice_idx + 1) % 20 == 0:\n",
    "        print(f\"  Processed {slice_idx + 1}/{num_slices} slices\")\n",
    "\n",
    "print(f\"Done! Processed {len(results)} slices.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Compute Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute temperature prediction metrics\n",
    "temp_mse_list = []\n",
    "temp_mae_list = []\n",
    "\n",
    "for result in results:\n",
    "    temp_pred = result[\"temp_pred\"][0]  # [1, H, W]\n",
    "    temp_target = result[\"temp_target\"]  # [1, H, W]\n",
    "    mask = result[\"temp_mask\"]  # [H, W]\n",
    "    \n",
    "    # Denormalize for MAE calculation\n",
    "    temp_pred_denorm = temp_test_dataset.denormalize(temp_pred)\n",
    "    temp_target_denorm = temp_test_dataset.denormalize(temp_target)\n",
    "    \n",
    "    # MSE on normalized data\n",
    "    mse = torch.nn.functional.mse_loss(temp_pred, temp_target).item()\n",
    "    \n",
    "    # MAE on denormalized data (masked)\n",
    "    if mask.sum() > 0:\n",
    "        mae = torch.abs(temp_pred_denorm[0][mask] - temp_target_denorm[0][mask]).mean().item()\n",
    "    else:\n",
    "        mae = torch.abs(temp_pred_denorm - temp_target_denorm).mean().item()\n",
    "    \n",
    "    temp_mse_list.append(mse)\n",
    "    temp_mae_list.append(mae)\n",
    "\n",
    "print(\"Temperature Prediction Metrics (all slices in timestep 18):\")\n",
    "print(f\"  Mean MSE (normalized): {np.mean(temp_mse_list):.6f} ± {np.std(temp_mse_list):.6f}\")\n",
    "print(f\"  Mean MAE (actual temp): {np.mean(temp_mae_list):.2f} ± {np.std(temp_mae_list):.2f} K\")\n",
    "print(f\"  Median MAE: {np.median(temp_mae_list):.2f} K\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute microstructure prediction metrics\n",
    "micro_mse_cascaded_list = []\n",
    "micro_mse_standard_list = []\n",
    "micro_mae_cascaded_list = []\n",
    "micro_mae_standard_list = []\n",
    "\n",
    "for result in results:\n",
    "    micro_pred_cascaded = result[\"micro_pred_cascaded\"][0]  # [10, H, W]\n",
    "    micro_pred_standard = result[\"micro_pred_standard\"][0]  # [10, H, W]\n",
    "    micro_target = result[\"micro_target\"]  # [10, H, W]\n",
    "    mask = result[\"micro_mask\"]  # [H, W]\n",
    "    \n",
    "    # MSE on normalized data\n",
    "    mse_cascaded = torch.nn.functional.mse_loss(micro_pred_cascaded, micro_target).item()\n",
    "    mse_standard = torch.nn.functional.mse_loss(micro_pred_standard, micro_target).item()\n",
    "    \n",
    "    # Denormalize for MAE calculation\n",
    "    micro_pred_cascaded_denorm = micro_test_dataset.denormalize_target(micro_pred_cascaded)\n",
    "    micro_pred_standard_denorm = micro_test_dataset.denormalize_target(micro_pred_standard)\n",
    "    micro_target_denorm = micro_test_dataset.denormalize_target(micro_target)\n",
    "    \n",
    "    # MAE on IPF-X channels (first 3) - these are the main visual channels\n",
    "    if mask.sum() > 0:\n",
    "        mae_cascaded = torch.abs(micro_pred_cascaded_denorm[0:3][:, mask] - micro_target_denorm[0:3][:, mask]).mean().item()\n",
    "        mae_standard = torch.abs(micro_pred_standard_denorm[0:3][:, mask] - micro_target_denorm[0:3][:, mask]).mean().item()\n",
    "    else:\n",
    "        mae_cascaded = torch.abs(micro_pred_cascaded_denorm[0:3] - micro_target_denorm[0:3]).mean().item()\n",
    "        mae_standard = torch.abs(micro_pred_standard_denorm[0:3] - micro_target_denorm[0:3]).mean().item()\n",
    "    \n",
    "    micro_mse_cascaded_list.append(mse_cascaded)\n",
    "    micro_mse_standard_list.append(mse_standard)\n",
    "    micro_mae_cascaded_list.append(mae_cascaded)\n",
    "    micro_mae_standard_list.append(mae_standard)\n",
    "\n",
    "print(\"Microstructure Prediction Metrics (all slices in timestep 18):\")\n",
    "print(\"\\n  CASCADED (using predicted temperature):\")\n",
    "print(f\"    Mean MSE (normalized): {np.mean(micro_mse_cascaded_list):.6f} ± {np.std(micro_mse_cascaded_list):.6f}\")\n",
    "print(f\"    Mean MAE (IPF-X channels): {np.mean(micro_mae_cascaded_list):.4f} ± {np.std(micro_mae_cascaded_list):.4f}\")\n",
    "print(\"\\n  STANDARD (using ground truth temperature):\")\n",
    "print(f\"    Mean MSE (normalized): {np.mean(micro_mse_standard_list):.6f} ± {np.std(micro_mse_standard_list):.6f}\")\n",
    "print(f\"    Mean MAE (IPF-X channels): {np.mean(micro_mae_standard_list):.4f} ± {np.std(micro_mae_standard_list):.4f}\")\n",
    "print(\"\\n  Difference (Cascaded - Standard):\")\n",
    "print(f\"    MSE difference: {np.mean(micro_mse_cascaded_list) - np.mean(micro_mse_standard_list):.6f}\")\n",
    "print(f\"    MAE difference: {np.mean(micro_mae_cascaded_list) - np.mean(micro_mae_standard_list):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Visualize Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select a representative slice to visualize (middle of the domain)\n",
    "viz_slice_idx = num_slices // 2\n",
    "result = results[viz_slice_idx]\n",
    "\n",
    "print(f\"Visualizing slice {viz_slice_idx} (Y-slice at middle of domain)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize temperature prediction\n",
    "temp_input_denorm = temp_test_dataset.denormalize(result[\"temp_input_seq\"])\n",
    "temp_target_denorm = temp_test_dataset.denormalize(result[\"temp_target\"])\n",
    "temp_pred_denorm = temp_test_dataset.denormalize(result[\"temp_pred\"][0])\n",
    "\n",
    "fig = plot_temperature_prediction(\n",
    "    input_seq=temp_input_denorm,\n",
    "    target=temp_target_denorm,\n",
    "    prediction=temp_pred_denorm,\n",
    "    title=f\"Temperature Prediction - Slice {viz_slice_idx}\",\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize microstructure prediction (CASCADED)\n",
    "micro_input_denorm = micro_test_dataset.denormalize(result[\"micro_input_seq\"])\n",
    "micro_target_denorm = micro_test_dataset.denormalize_target(result[\"micro_target\"])\n",
    "micro_pred_cascaded_denorm = micro_test_dataset.denormalize_target(result[\"micro_pred_cascaded\"][0])\n",
    "\n",
    "fig = plot_microstructure_prediction(\n",
    "    input_seq=micro_input_denorm,\n",
    "    target=micro_target_denorm,\n",
    "    prediction=micro_pred_cascaded_denorm,\n",
    "    title=f\"Microstructure Prediction (CASCADED) - Slice {viz_slice_idx}\",\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize microstructure prediction (STANDARD - using ground truth temperature)\n",
    "micro_pred_standard_denorm = micro_test_dataset.denormalize_target(result[\"micro_pred_standard\"][0])\n",
    "\n",
    "fig = plot_microstructure_prediction(\n",
    "    input_seq=micro_input_denorm,\n",
    "    target=micro_target_denorm,\n",
    "    prediction=micro_pred_standard_denorm,\n",
    "    title=f\"Microstructure Prediction (STANDARD) - Slice {viz_slice_idx}\",\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare cascaded vs standard microstructure predictions side by side\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "\n",
    "# Top row: Ground truth and cascaded prediction\n",
    "# IPF-X RGB visualization\n",
    "target_rgb = np.clip(np.transpose(micro_target_denorm[0:3].numpy(), (2, 1, 0)), 0, 1).astype(np.float32)\n",
    "cascaded_rgb = np.clip(np.transpose(micro_pred_cascaded_denorm[0:3].numpy(), (2, 1, 0)), 0, 1).astype(np.float32)\n",
    "standard_rgb = np.clip(np.transpose(micro_pred_standard_denorm[0:3].numpy(), (2, 1, 0)), 0, 1).astype(np.float32)\n",
    "\n",
    "axes[0, 0].imshow(target_rgb, aspect='equal', origin='lower')\n",
    "axes[0, 0].set_title('Ground Truth')\n",
    "axes[0, 0].set_xlabel('X coordinate')\n",
    "axes[0, 0].set_ylabel('Z coordinate')\n",
    "\n",
    "axes[0, 1].imshow(cascaded_rgb, aspect='equal', origin='lower')\n",
    "axes[0, 1].set_title('Cascaded Prediction\\n(using predicted temperature)')\n",
    "axes[0, 1].set_xlabel('X coordinate')\n",
    "axes[0, 1].set_ylabel('Z coordinate')\n",
    "\n",
    "axes[0, 2].imshow(standard_rgb, aspect='equal', origin='lower')\n",
    "axes[0, 2].set_title('Standard Prediction\\n(using ground truth temperature)')\n",
    "axes[0, 2].set_xlabel('X coordinate')\n",
    "axes[0, 2].set_ylabel('Z coordinate')\n",
    "\n",
    "# Bottom row: Error maps\n",
    "cascaded_error = np.mean((micro_target_denorm[0:3].numpy() - micro_pred_cascaded_denorm[0:3].numpy()) ** 2, axis=0).astype(np.float32)\n",
    "standard_error = np.mean((micro_target_denorm[0:3].numpy() - micro_pred_standard_denorm[0:3].numpy()) ** 2, axis=0).astype(np.float32)\n",
    "error_diff = cascaded_error - standard_error  # Positive = cascaded is worse\n",
    "\n",
    "# Use same color scale for error maps\n",
    "vmax_error = max(cascaded_error.max(), standard_error.max())\n",
    "\n",
    "im0 = axes[1, 0].imshow(cascaded_error.T, cmap='RdYlBu_r', aspect='equal', origin='lower', vmin=0, vmax=vmax_error)\n",
    "axes[1, 0].set_title(f'Cascaded Error (MSE: {cascaded_error.mean():.4f})')\n",
    "axes[1, 0].set_xlabel('X coordinate')\n",
    "axes[1, 0].set_ylabel('Z coordinate')\n",
    "plt.colorbar(im0, ax=axes[1, 0], fraction=0.046, pad=0.04)\n",
    "\n",
    "im1 = axes[1, 1].imshow(standard_error.T, cmap='RdYlBu_r', aspect='equal', origin='lower', vmin=0, vmax=vmax_error)\n",
    "axes[1, 1].set_title(f'Standard Error (MSE: {standard_error.mean():.4f})')\n",
    "axes[1, 1].set_xlabel('X coordinate')\n",
    "axes[1, 1].set_ylabel('Z coordinate')\n",
    "plt.colorbar(im1, ax=axes[1, 1], fraction=0.046, pad=0.04)\n",
    "\n",
    "# Difference map (positive = cascaded is worse)\n",
    "vmax_diff = max(abs(error_diff.min()), abs(error_diff.max()))\n",
    "im2 = axes[1, 2].imshow(error_diff.T, cmap='RdBu_r', aspect='equal', origin='lower', vmin=-vmax_diff, vmax=vmax_diff)\n",
    "axes[1, 2].set_title(f'Error Difference\\n(Cascaded - Standard)')\n",
    "axes[1, 2].set_xlabel('X coordinate')\n",
    "axes[1, 2].set_ylabel('Z coordinate')\n",
    "plt.colorbar(im2, ax=axes[1, 2], fraction=0.046, pad=0.04)\n",
    "\n",
    "plt.suptitle(f'Cascaded vs Standard Microstructure Prediction - Slice {viz_slice_idx}', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.savefig('cascaded_vs_standard_comparison.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Aggregate Metrics Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot MSE distribution across slices\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "# Temperature MSE\n",
    "axes[0].hist(temp_mse_list, bins=20, edgecolor='black', alpha=0.7, color='coral')\n",
    "axes[0].axvline(np.mean(temp_mse_list), color='red', linestyle='--', linewidth=2, label=f'Mean: {np.mean(temp_mse_list):.6f}')\n",
    "axes[0].set_xlabel('MSE (normalized)')\n",
    "axes[0].set_ylabel('Frequency')\n",
    "axes[0].set_title('Temperature MSE Distribution')\n",
    "axes[0].legend()\n",
    "axes[0].grid(alpha=0.3)\n",
    "\n",
    "# Microstructure MSE comparison\n",
    "x = np.arange(len(micro_mse_cascaded_list))\n",
    "axes[1].scatter(x, micro_mse_cascaded_list, alpha=0.6, label='Cascaded', s=10)\n",
    "axes[1].scatter(x, micro_mse_standard_list, alpha=0.6, label='Standard', s=10)\n",
    "axes[1].axhline(np.mean(micro_mse_cascaded_list), color='blue', linestyle='--', alpha=0.7)\n",
    "axes[1].axhline(np.mean(micro_mse_standard_list), color='orange', linestyle='--', alpha=0.7)\n",
    "axes[1].set_xlabel('Slice Index')\n",
    "axes[1].set_ylabel('MSE (normalized)')\n",
    "axes[1].set_title('Microstructure MSE by Slice')\n",
    "axes[1].legend()\n",
    "axes[1].grid(alpha=0.3)\n",
    "\n",
    "# MSE difference\n",
    "mse_diff = np.array(micro_mse_cascaded_list) - np.array(micro_mse_standard_list)\n",
    "axes[2].bar(x, mse_diff, alpha=0.7, color=np.where(mse_diff > 0, 'red', 'green'))\n",
    "axes[2].axhline(0, color='black', linewidth=1)\n",
    "axes[2].axhline(np.mean(mse_diff), color='purple', linestyle='--', linewidth=2, label=f'Mean diff: {np.mean(mse_diff):.6f}')\n",
    "axes[2].set_xlabel('Slice Index')\n",
    "axes[2].set_ylabel('MSE Difference (Cascaded - Standard)')\n",
    "axes[2].set_title('Microstructure MSE Difference')\n",
    "axes[2].legend()\n",
    "axes[2].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('mse_distribution.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Summary\n",
    "print(f\"\\nSummary:\")\n",
    "print(f\"  Slices where cascaded is WORSE: {np.sum(mse_diff > 0)}/{len(mse_diff)}\")\n",
    "print(f\"  Slices where cascaded is BETTER: {np.sum(mse_diff < 0)}/{len(mse_diff)}\")\n",
    "print(f\"  Mean MSE increase from using predicted temperature: {np.mean(mse_diff):.6f} ({np.mean(mse_diff)/np.mean(micro_mse_standard_list)*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Visualize Multiple Slices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize a few representative slices\n",
    "slice_indices = [0, num_slices // 4, num_slices // 2, 3 * num_slices // 4, num_slices - 1]\n",
    "\n",
    "fig, axes = plt.subplots(len(slice_indices), 4, figsize=(16, 4 * len(slice_indices)))\n",
    "\n",
    "for row, slice_idx in enumerate(slice_indices):\n",
    "    result = results[slice_idx]\n",
    "    \n",
    "    # Get denormalized data\n",
    "    micro_target_denorm = micro_test_dataset.denormalize_target(result[\"micro_target\"])\n",
    "    micro_pred_cascaded_denorm = micro_test_dataset.denormalize_target(result[\"micro_pred_cascaded\"][0])\n",
    "    micro_pred_standard_denorm = micro_test_dataset.denormalize_target(result[\"micro_pred_standard\"][0])\n",
    "    \n",
    "    # RGB visualization\n",
    "    target_rgb = np.clip(np.transpose(micro_target_denorm[0:3].numpy(), (2, 1, 0)), 0, 1).astype(np.float32)\n",
    "    cascaded_rgb = np.clip(np.transpose(micro_pred_cascaded_denorm[0:3].numpy(), (2, 1, 0)), 0, 1).astype(np.float32)\n",
    "    standard_rgb = np.clip(np.transpose(micro_pred_standard_denorm[0:3].numpy(), (2, 1, 0)), 0, 1).astype(np.float32)\n",
    "    \n",
    "    # Error\n",
    "    cascaded_error = np.mean((micro_target_denorm[0:3].numpy() - micro_pred_cascaded_denorm[0:3].numpy()) ** 2, axis=0).astype(np.float32)\n",
    "    \n",
    "    axes[row, 0].imshow(target_rgb, aspect='equal', origin='lower')\n",
    "    axes[row, 0].set_title(f'Slice {slice_idx}: Ground Truth')\n",
    "    axes[row, 0].set_ylabel('Z')\n",
    "    \n",
    "    axes[row, 1].imshow(cascaded_rgb, aspect='equal', origin='lower')\n",
    "    axes[row, 1].set_title('Cascaded Prediction')\n",
    "    \n",
    "    axes[row, 2].imshow(standard_rgb, aspect='equal', origin='lower')\n",
    "    axes[row, 2].set_title('Standard Prediction')\n",
    "    \n",
    "    im = axes[row, 3].imshow(cascaded_error.T, cmap='RdYlBu_r', aspect='equal', origin='lower')\n",
    "    axes[row, 3].set_title(f'Cascaded Error (MSE: {cascaded_error.mean():.4f})')\n",
    "    plt.colorbar(im, ax=axes[row, 3], fraction=0.046, pad=0.04)\n",
    "\n",
    "for ax in axes[-1, :]:\n",
    "    ax.set_xlabel('X')\n",
    "\n",
    "plt.suptitle('Cascaded Predictions Across Slices (Timestep 18)', fontsize=14, y=1.01)\n",
    "plt.tight_layout()\n",
    "plt.savefig('cascaded_predictions_all_slices.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Autoregressive Multi-Timestep Prediction\n",
    "\n",
    "In this section, we perform autoregressive prediction where the output from one timestep is used as input for the next timestep. This tests how errors accumulate over time in a realistic deployment scenario.\n",
    "\n",
    "**Pipeline for each timestep:**\n",
    "1. Predict temperature using the previous 3 temperature frames (either from ground truth or previous predictions)\n",
    "2. Use the predicted temperature + previous microstructure frames to predict the next microstructure\n",
    "3. Store predictions and use them as inputs for subsequent timesteps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration for autoregressive prediction\n",
    "num_autoregressive_steps = 3  # Number of timesteps to predict autoregressively\n",
    "sequence_length = 3  # Input sequence length required by the model\n",
    "start_timestep = 18  # First test timestep (relative timestep 0 in test split)\n",
    "selected_slice = num_slices // 2  # Use middle slice for demonstration\n",
    "\n",
    "print(f\"Autoregressive prediction settings:\")\n",
    "print(f\"  Starting timestep: {start_timestep}\")\n",
    "print(f\"  Number of autoregressive steps: {num_autoregressive_steps}\")\n",
    "print(f\"  Selected slice: {selected_slice} (middle of domain)\")\n",
    "print(f\"  Sequence length: {sequence_length}\")\n",
    "print(f\"  Will predict timesteps: {start_timestep + 1} to {start_timestep + num_autoregressive_steps}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_ground_truth_frame(temp_test_dataset, micro_test_dataset, slice_idx, temporal_offset):\n",
    "    \"\"\"\n",
    "    Load a single frame of ground truth data at a specific temporal offset.\n",
    "    \n",
    "    Args:\n",
    "        temp_dataset: Temperature dataset\n",
    "        micro_dataset: Microstructure dataset\n",
    "        slice_idx: Slice index (0-93 for xz plane)\n",
    "        temporal_offset: Temporal offset within the test split (0 = timestep 18)\n",
    "    \n",
    "    Returns:\n",
    "        temp_frame: [1, H, W] normalized temperature\n",
    "        micro_frame: [11, H, W] normalized microstructure (10 channels + 1 temperature)\n",
    "    \"\"\"\n",
    "    # Calculate dataset index: slice_idx + temporal_offset * num_slices\n",
    "    # But we need to be careful about how the dataset indexes frames\n",
    "    # The dataset returns sequences, so we need to access the underlying data\n",
    "    \n",
    "    # Access underlying normalized data directly\n",
    "    # Shape is [total_frames, channels, H, W] where total_frames = timesteps * slices\n",
    "    frame_idx = temporal_offset * num_slices + slice_idx\n",
    "    \n",
    "    temp_frame = temp_test_dataset.data[frame_idx]  # [1, H, W]\n",
    "    micro_frame = micro_test_dataset.data[frame_idx]  # [11, H, W]\n",
    "    \n",
    "    return temp_frame, micro_frame\n",
    "\n",
    "\n",
    "def autoregressive_cascaded_prediction(\n",
    "    temp_model,\n",
    "    micro_model,\n",
    "    temp_dataset,\n",
    "    micro_dataset,\n",
    "    slice_idx,\n",
    "    num_steps,\n",
    "    sequence_length,\n",
    "    device,\n",
    "):\n",
    "    \"\"\"\n",
    "    Perform autoregressive cascaded prediction over multiple timesteps.\n",
    "    \n",
    "    For each timestep:\n",
    "    1. Build input sequence from previous frames (either GT or predicted)\n",
    "    2. Predict temperature\n",
    "    3. Predict microstructure using predicted temperature\n",
    "    4. Store predictions for use in subsequent timesteps\n",
    "    \n",
    "    Args:\n",
    "        temp_model: Temperature prediction model\n",
    "        micro_model: Microstructure prediction model\n",
    "        temp_dataset: Temperature test dataset\n",
    "        micro_dataset: Microstructure test dataset\n",
    "        slice_idx: Slice index\n",
    "        num_steps: Number of timesteps to predict\n",
    "        sequence_length: Required input sequence length\n",
    "        device: Torch device\n",
    "        \n",
    "    Returns:\n",
    "        dict with predictions, ground truths, and metrics for each timestep\n",
    "    \"\"\"\n",
    "    results = {\n",
    "        \"temp_predictions\": [],\n",
    "        \"temp_targets\": [],\n",
    "        \"micro_predictions\": [],\n",
    "        \"micro_targets\": [],\n",
    "        \"temp_mse\": [],\n",
    "        \"micro_mse\": [],\n",
    "    }\n",
    "    \n",
    "    # Initialize buffers to store predicted frames for autoregressive prediction\n",
    "    # These will be used to build input sequences for future predictions\n",
    "    temp_pred_buffer = []  # List of [1, H, W] tensors\n",
    "    micro_pred_buffer = []  # List of [10, H, W] tensors (microstructure only, no temp)\n",
    "    \n",
    "    for step in range(num_steps):\n",
    "        # Current temporal offset: we're predicting frame at (sequence_length + step)\n",
    "        # Input sequence spans temporal offsets: [step, step+1, ..., step+sequence_length-1]\n",
    "        # Target is at temporal offset: step + sequence_length\n",
    "        \n",
    "        target_temporal_offset = step + sequence_length\n",
    "        \n",
    "        # ===== Build temperature input sequence =====\n",
    "        temp_input_frames = []\n",
    "        for seq_idx in range(sequence_length):\n",
    "            temporal_offset = step + seq_idx\n",
    "            \n",
    "            if temporal_offset < sequence_length:\n",
    "                # Use ground truth for initial frames\n",
    "                temp_frame, _ = load_ground_truth_frame(\n",
    "                    temp_dataset, micro_dataset, slice_idx, temporal_offset\n",
    "                )\n",
    "            else:\n",
    "                # Use predicted frame\n",
    "                pred_idx = temporal_offset - sequence_length\n",
    "                temp_frame = temp_pred_buffer[pred_idx]\n",
    "            \n",
    "            temp_input_frames.append(temp_frame)\n",
    "        \n",
    "        temp_input_seq = torch.stack(temp_input_frames, dim=0)  # [seq_len, 1, H, W]\n",
    "        \n",
    "        # ===== Build microstructure input sequence =====\n",
    "        micro_input_frames = []\n",
    "        for seq_idx in range(sequence_length):\n",
    "            temporal_offset = step + seq_idx\n",
    "            \n",
    "            if temporal_offset < sequence_length:\n",
    "                # Use ground truth for initial frames\n",
    "                _, micro_frame = load_ground_truth_frame(\n",
    "                    temp_dataset, micro_dataset, slice_idx, temporal_offset\n",
    "                )\n",
    "            else:\n",
    "                # Use predicted microstructure + temperature for this frame\n",
    "                pred_idx = temporal_offset - sequence_length\n",
    "                micro_channels = micro_pred_buffer[pred_idx]  # [10, H, W]\n",
    "                temp_channel = temp_pred_buffer[pred_idx]  # [1, H, W]\n",
    "                \n",
    "                # Renormalize temperature from temp normalizer to micro normalizer\n",
    "                temp_denorm = temp_dataset.denormalize(temp_channel)\n",
    "                if micro_dataset.normalizer is not None:\n",
    "                    temp_min = micro_dataset.normalizer.channel_mins[10].item()\n",
    "                    temp_max = micro_dataset.normalizer.channel_maxs[10].item()\n",
    "                    temp_renorm = (temp_denorm - temp_min) / (temp_max - temp_min)\n",
    "                else:\n",
    "                    temp_renorm = temp_denorm\n",
    "                \n",
    "                micro_frame = torch.cat([micro_channels, temp_renorm], dim=0)  # [11, H, W]\n",
    "            \n",
    "            micro_input_frames.append(micro_frame)\n",
    "        \n",
    "        micro_input_seq = torch.stack(micro_input_frames, dim=0)  # [seq_len, 11, H, W]\n",
    "        \n",
    "        # ===== Predict temperature =====\n",
    "        temp_input_batch = temp_input_seq.unsqueeze(0).half().to(device)\n",
    "        with torch.no_grad():\n",
    "            temp_pred = temp_model(temp_input_batch)  # [1, 1, H, W]\n",
    "        temp_pred = temp_pred[0].cpu().float()  # [1, H, W]\n",
    "        \n",
    "        # ===== Prepare microstructure input with predicted temperature =====\n",
    "        # Replace temperature in the last frame with predicted temperature\n",
    "        temp_pred_denorm = temp_dataset.denormalize(temp_pred)\n",
    "        if micro_dataset.normalizer is not None:\n",
    "            temp_min = micro_dataset.normalizer.channel_mins[10].item()\n",
    "            temp_max = micro_dataset.normalizer.channel_maxs[10].item()\n",
    "            temp_pred_norm_micro = (temp_pred_denorm - temp_min) / (temp_max - temp_min)\n",
    "        else:\n",
    "            temp_pred_norm_micro = temp_pred_denorm\n",
    "        \n",
    "        micro_input_cascaded = micro_input_seq.clone()\n",
    "        micro_input_cascaded[-1, 10:11, :, :] = temp_pred_norm_micro\n",
    "        \n",
    "        # ===== Predict microstructure =====\n",
    "        micro_input_batch = micro_input_cascaded.unsqueeze(0).half().to(device)\n",
    "        with torch.no_grad():\n",
    "            micro_pred = micro_model(micro_input_batch)  # [1, 10, H, W]\n",
    "        micro_pred = micro_pred[0].cpu().float()  # [10, H, W]\n",
    "        \n",
    "        # ===== Load ground truth targets =====\n",
    "        temp_target, micro_target_full = load_ground_truth_frame(\n",
    "            temp_dataset, micro_dataset, slice_idx, target_temporal_offset\n",
    "        )\n",
    "        micro_target = micro_target_full[:10]  # [10, H, W] - exclude temperature channel\n",
    "        \n",
    "        # ===== Store predictions in buffer for future steps =====\n",
    "        temp_pred_buffer.append(temp_pred)\n",
    "        micro_pred_buffer.append(micro_pred)\n",
    "        \n",
    "        # ===== Compute metrics =====\n",
    "        temp_mse = torch.nn.functional.mse_loss(temp_pred, temp_target).item()\n",
    "        micro_mse = torch.nn.functional.mse_loss(micro_pred, micro_target).item()\n",
    "        \n",
    "        # ===== Store results =====\n",
    "        results[\"temp_predictions\"].append(temp_pred)\n",
    "        results[\"temp_targets\"].append(temp_target)\n",
    "        results[\"micro_predictions\"].append(micro_pred)\n",
    "        results[\"micro_targets\"].append(micro_target)\n",
    "        results[\"temp_mse\"].append(temp_mse)\n",
    "        results[\"micro_mse\"].append(micro_mse)\n",
    "        \n",
    "        print(f\"  Step {step + 1}/{num_steps} (predicting t={start_timestep + target_temporal_offset}): \"\n",
    "              f\"Temp MSE={temp_mse:.6f}, Micro MSE={micro_mse:.6f}\")\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run autoregressive cascaded prediction\n",
    "print(f\"Running autoregressive cascaded prediction for slice {selected_slice}...\")\n",
    "print(f\"Using ground truth frames from timesteps {start_timestep} to {start_timestep + sequence_length - 1} as initial context\")\n",
    "print()\n",
    "\n",
    "autoregressive_results = autoregressive_cascaded_prediction(\n",
    "    temp_model=temp_model,\n",
    "    micro_model=micro_model,\n",
    "    temp_dataset=temp_test_dataset,\n",
    "    micro_dataset=micro_test_dataset,\n",
    "    slice_idx=selected_slice,\n",
    "    num_steps=num_autoregressive_steps,\n",
    "    sequence_length=sequence_length,\n",
    "    device=device,\n",
    ")\n",
    "\n",
    "print()\n",
    "print(\"Autoregressive prediction complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.1 Analyze Error Accumulation Over Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot error accumulation over autoregressive steps\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "steps = np.arange(1, num_autoregressive_steps + 1)\n",
    "timesteps = [start_timestep + sequence_length + i for i in range(num_autoregressive_steps)]\n",
    "\n",
    "# Temperature MSE over steps\n",
    "axes[0].plot(steps, autoregressive_results[\"temp_mse\"], 'o-', color='coral', linewidth=2, markersize=8)\n",
    "axes[0].set_xlabel('Autoregressive Step')\n",
    "axes[0].set_ylabel('MSE (normalized)')\n",
    "axes[0].set_title('Temperature Prediction Error Over Time')\n",
    "axes[0].set_xticks(steps)\n",
    "axes[0].set_xticklabels([f\"Step {s}\\n(t={t})\" for s, t in zip(steps, timesteps)])\n",
    "axes[0].grid(alpha=0.3)\n",
    "\n",
    "# Add percentage increase annotation\n",
    "if len(autoregressive_results[\"temp_mse\"]) > 1:\n",
    "    first_mse = autoregressive_results[\"temp_mse\"][0]\n",
    "    last_mse = autoregressive_results[\"temp_mse\"][-1]\n",
    "    pct_increase = ((last_mse - first_mse) / first_mse) * 100\n",
    "    axes[0].annotate(f'{pct_increase:+.1f}% from step 1',\n",
    "                     xy=(steps[-1], last_mse),\n",
    "                     xytext=(steps[-1] - 0.5, last_mse * 1.2),\n",
    "                     fontsize=10, ha='center')\n",
    "\n",
    "# Microstructure MSE over steps\n",
    "axes[1].plot(steps, autoregressive_results[\"micro_mse\"], 's-', color='steelblue', linewidth=2, markersize=8)\n",
    "axes[1].set_xlabel('Autoregressive Step')\n",
    "axes[1].set_ylabel('MSE (normalized)')\n",
    "axes[1].set_title('Microstructure Prediction Error Over Time')\n",
    "axes[1].set_xticks(steps)\n",
    "axes[1].set_xticklabels([f\"Step {s}\\n(t={t})\" for s, t in zip(steps, timesteps)])\n",
    "axes[1].grid(alpha=0.3)\n",
    "\n",
    "# Add percentage increase annotation\n",
    "if len(autoregressive_results[\"micro_mse\"]) > 1:\n",
    "    first_mse = autoregressive_results[\"micro_mse\"][0]\n",
    "    last_mse = autoregressive_results[\"micro_mse\"][-1]\n",
    "    pct_increase = ((last_mse - first_mse) / first_mse) * 100\n",
    "    axes[1].annotate(f'{pct_increase:+.1f}% from step 1',\n",
    "                     xy=(steps[-1], last_mse),\n",
    "                     xytext=(steps[-1] - 0.5, last_mse * 1.2),\n",
    "                     fontsize=10, ha='center')\n",
    "\n",
    "plt.suptitle(f'Error Accumulation in Autoregressive Prediction (Slice {selected_slice})', fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.savefig('autoregressive_error_accumulation.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Print summary statistics\n",
    "print(\"\\nError Accumulation Summary:\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"{'Step':<8} {'Timestep':<10} {'Temp MSE':<15} {'Micro MSE':<15}\")\n",
    "print(\"-\" * 50)\n",
    "for i, (t_mse, m_mse) in enumerate(zip(autoregressive_results[\"temp_mse\"], autoregressive_results[\"micro_mse\"])):\n",
    "    print(f\"{i+1:<8} {timesteps[i]:<10} {t_mse:<15.6f} {m_mse:<15.6f}\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"{'Total increase:':<18} {autoregressive_results['temp_mse'][-1] - autoregressive_results['temp_mse'][0]:<15.6f} \"\n",
    "      f\"{autoregressive_results['micro_mse'][-1] - autoregressive_results['micro_mse'][0]:<15.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.2 Visualize Predictions at Each Autoregressive Step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize temperature predictions at each autoregressive step\n",
    "fig, axes = plt.subplots(num_autoregressive_steps, 3, figsize=(12, 4 * num_autoregressive_steps))\n",
    "\n",
    "for step in range(num_autoregressive_steps):\n",
    "    # Denormalize for visualization\n",
    "    temp_pred = autoregressive_results[\"temp_predictions\"][step]\n",
    "    temp_target = autoregressive_results[\"temp_targets\"][step]\n",
    "    \n",
    "    temp_pred_denorm = temp_test_dataset.denormalize(temp_pred).numpy()[0]\n",
    "    temp_target_denorm = temp_test_dataset.denormalize(temp_target).numpy()[0]\n",
    "    temp_error = np.abs(temp_pred_denorm - temp_target_denorm)\n",
    "    \n",
    "    timestep = start_timestep + sequence_length + step\n",
    "    \n",
    "    # Shared colorbar limits for prediction and target\n",
    "    vmin = min(temp_pred_denorm.min(), temp_target_denorm.min())\n",
    "    vmax = max(temp_pred_denorm.max(), temp_target_denorm.max())\n",
    "    \n",
    "    # Ground truth\n",
    "    im0 = axes[step, 0].imshow(temp_target_denorm.T, cmap='hot', aspect='equal', origin='lower', vmin=vmin, vmax=vmax)\n",
    "    axes[step, 0].set_title(f'Step {step+1} (t={timestep}): Ground Truth')\n",
    "    axes[step, 0].set_ylabel('Z')\n",
    "    plt.colorbar(im0, ax=axes[step, 0], fraction=0.046, pad=0.04, label='T [K]')\n",
    "    \n",
    "    # Prediction\n",
    "    im1 = axes[step, 1].imshow(temp_pred_denorm.T, cmap='hot', aspect='equal', origin='lower', vmin=vmin, vmax=vmax)\n",
    "    axes[step, 1].set_title(f'Prediction (MSE: {autoregressive_results[\"temp_mse\"][step]:.6f})')\n",
    "    plt.colorbar(im1, ax=axes[step, 1], fraction=0.046, pad=0.04, label='T [K]')\n",
    "    \n",
    "    # Error\n",
    "    im2 = axes[step, 2].imshow(temp_error.T, cmap='RdYlBu_r', aspect='equal', origin='lower')\n",
    "    axes[step, 2].set_title(f'Absolute Error (MAE: {temp_error.mean():.2f} K)')\n",
    "    plt.colorbar(im2, ax=axes[step, 2], fraction=0.046, pad=0.04, label='|Error| [K]')\n",
    "\n",
    "for ax in axes[-1, :]:\n",
    "    ax.set_xlabel('X')\n",
    "\n",
    "plt.suptitle(f'Temperature Predictions - Autoregressive Steps (Slice {selected_slice})', fontsize=14, y=1.01)\n",
    "plt.tight_layout()\n",
    "plt.savefig('autoregressive_temperature_predictions.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize microstructure predictions at each autoregressive step\n",
    "fig, axes = plt.subplots(num_autoregressive_steps, 4, figsize=(16, 4 * num_autoregressive_steps))\n",
    "\n",
    "for step in range(num_autoregressive_steps):\n",
    "    # Denormalize for visualization\n",
    "    micro_pred = autoregressive_results[\"micro_predictions\"][step]\n",
    "    micro_target = autoregressive_results[\"micro_targets\"][step]\n",
    "    \n",
    "    micro_pred_denorm = micro_test_dataset.denormalize_target(micro_pred)\n",
    "    micro_target_denorm = micro_test_dataset.denormalize_target(micro_target)\n",
    "    \n",
    "    timestep = start_timestep + sequence_length + step\n",
    "    \n",
    "    # IPF-X RGB visualization (first 3 channels)\n",
    "    target_rgb = np.clip(np.transpose(micro_target_denorm[0:3].numpy(), (2, 1, 0)), 0, 1).astype(np.float32)\n",
    "    pred_rgb = np.clip(np.transpose(micro_pred_denorm[0:3].numpy(), (2, 1, 0)), 0, 1).astype(np.float32)\n",
    "    \n",
    "    # Compute error\n",
    "    error = np.mean((micro_target_denorm[0:3].numpy() - micro_pred_denorm[0:3].numpy()) ** 2, axis=0).astype(np.float32)\n",
    "    \n",
    "    # Difference image (scaled for visibility)\n",
    "    diff = np.abs(target_rgb - pred_rgb)\n",
    "    diff_scaled = np.clip(diff * 5, 0, 1)  # Scale up for visibility\n",
    "    \n",
    "    # Ground truth\n",
    "    axes[step, 0].imshow(target_rgb, aspect='equal', origin='lower')\n",
    "    axes[step, 0].set_title(f'Step {step+1} (t={timestep}): Ground Truth')\n",
    "    axes[step, 0].set_ylabel('Z')\n",
    "    \n",
    "    # Prediction\n",
    "    axes[step, 1].imshow(pred_rgb, aspect='equal', origin='lower')\n",
    "    axes[step, 1].set_title(f'Prediction (MSE: {autoregressive_results[\"micro_mse\"][step]:.6f})')\n",
    "    \n",
    "    # Difference (scaled)\n",
    "    axes[step, 2].imshow(diff_scaled, aspect='equal', origin='lower')\n",
    "    axes[step, 2].set_title('|Difference| (5x scaled)')\n",
    "    \n",
    "    # MSE error map\n",
    "    im = axes[step, 3].imshow(error.T, cmap='RdYlBu_r', aspect='equal', origin='lower')\n",
    "    axes[step, 3].set_title(f'MSE Error Map')\n",
    "    plt.colorbar(im, ax=axes[step, 3], fraction=0.046, pad=0.04)\n",
    "\n",
    "for ax in axes[-1, :]:\n",
    "    ax.set_xlabel('X')\n",
    "\n",
    "plt.suptitle(f'Microstructure Predictions (IPF-X) - Autoregressive Steps (Slice {selected_slice})', fontsize=14, y=1.01)\n",
    "plt.tight_layout()\n",
    "plt.savefig('autoregressive_microstructure_predictions.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.3 Compare Autoregressive vs Single-Step Predictions\n",
    "\n",
    "Compare the quality of predictions from the autoregressive approach against single-step predictions (where ground truth is always used as input)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run single-step predictions for the same timesteps (using ground truth inputs)\n",
    "# This serves as a baseline to measure the impact of error accumulation\n",
    "\n",
    "single_step_temp_mse = []\n",
    "single_step_micro_mse = []\n",
    "\n",
    "print(\"Running single-step predictions for comparison (using ground truth inputs)...\")\n",
    "for step in range(num_autoregressive_steps):\n",
    "    # Calculate dataset index for this timestep\n",
    "    # For step 0, we want temporal_offset = sequence_length (predicting t=21 from t=[18,19,20])\n",
    "    # Dataset index = slice_idx + temporal_offset * num_slices\n",
    "    # But the dataset already handles sequences, so we need the correct starting index\n",
    "    \n",
    "    # Actually, for single-step, we can just use the test dataset directly\n",
    "    # Index in test dataset corresponds to: slice + (temporal_offset - sequence_length) * num_slices\n",
    "    # Wait, let me reconsider: the dataset getitem returns sequences ending at different temporal positions\n",
    "    \n",
    "    # The dataset index for getting a sequence ending at temporal_offset T is:\n",
    "    # idx = slice_idx + (T - sequence_length) * num_slices\n",
    "    # So for step 0 (T=sequence_length=3), idx = slice_idx + 0 * num_slices = slice_idx\n",
    "    # For step 1 (T=4), idx = slice_idx + 1 * num_slices\n",
    "    \n",
    "    temporal_target = sequence_length + step  # 3, 4, 5, ...\n",
    "    dataset_idx = selected_slice + step * num_slices\n",
    "    \n",
    "    # Get temperature prediction\n",
    "    temp_input, temp_target, _, temp_mask = temp_test_dataset[dataset_idx]\n",
    "    temp_input_batch = temp_input.unsqueeze(0).half().to(device)\n",
    "    with torch.no_grad():\n",
    "        temp_pred = temp_model(temp_input_batch)\n",
    "    temp_mse = torch.nn.functional.mse_loss(temp_pred[0].cpu().float(), temp_target).item()\n",
    "    single_step_temp_mse.append(temp_mse)\n",
    "    \n",
    "    # Get microstructure prediction (standard - using GT temperature)\n",
    "    micro_input, micro_target, _, micro_mask = micro_test_dataset[dataset_idx]\n",
    "    micro_input_batch = micro_input.unsqueeze(0).half().to(device)\n",
    "    with torch.no_grad():\n",
    "        micro_pred = micro_model(micro_input_batch)\n",
    "    micro_mse = torch.nn.functional.mse_loss(micro_pred[0].cpu().float(), micro_target).item()\n",
    "    single_step_micro_mse.append(micro_mse)\n",
    "    \n",
    "    timestep = start_timestep + temporal_target\n",
    "    print(f\"  Step {step + 1}: t={timestep}, Temp MSE={temp_mse:.6f}, Micro MSE={micro_mse:.6f}\")\n",
    "\n",
    "print(\"\\nSingle-step predictions complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot comparison: Autoregressive vs Single-Step\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "steps = np.arange(1, num_autoregressive_steps + 1)\n",
    "timesteps = [start_timestep + sequence_length + i for i in range(num_autoregressive_steps)]\n",
    "\n",
    "# Temperature comparison\n",
    "axes[0].plot(steps, autoregressive_results[\"temp_mse\"], 'o-', color='coral', linewidth=2, markersize=10, label='Autoregressive')\n",
    "axes[0].plot(steps, single_step_temp_mse, 's--', color='darkred', linewidth=2, markersize=10, label='Single-step (GT input)')\n",
    "axes[0].set_xlabel('Prediction Step')\n",
    "axes[0].set_ylabel('MSE (normalized)')\n",
    "axes[0].set_title('Temperature: Autoregressive vs Single-Step')\n",
    "axes[0].set_xticks(steps)\n",
    "axes[0].set_xticklabels([f\"Step {s}\\n(t={t})\" for s, t in zip(steps, timesteps)])\n",
    "axes[0].legend()\n",
    "axes[0].grid(alpha=0.3)\n",
    "\n",
    "# Microstructure comparison\n",
    "axes[1].plot(steps, autoregressive_results[\"micro_mse\"], 'o-', color='steelblue', linewidth=2, markersize=10, label='Autoregressive')\n",
    "axes[1].plot(steps, single_step_micro_mse, 's--', color='darkblue', linewidth=2, markersize=10, label='Single-step (GT input)')\n",
    "axes[1].set_xlabel('Prediction Step')\n",
    "axes[1].set_ylabel('MSE (normalized)')\n",
    "axes[1].set_title('Microstructure: Autoregressive vs Single-Step')\n",
    "axes[1].set_xticks(steps)\n",
    "axes[1].set_xticklabels([f\"Step {s}\\n(t={t})\" for s, t in zip(steps, timesteps)])\n",
    "axes[1].legend()\n",
    "axes[1].grid(alpha=0.3)\n",
    "\n",
    "plt.suptitle(f'Autoregressive vs Single-Step Prediction Comparison (Slice {selected_slice})', fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.savefig('autoregressive_vs_single_step.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Calculate degradation due to autoregressive error accumulation\n",
    "print(\"\\nError Degradation Analysis:\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"{'Step':<8} {'Timestep':<10} {'Temp (AR)':<12} {'Temp (SS)':<12} {'Δ Temp':<12} {'Micro (AR)':<12} {'Micro (SS)':<12} {'Δ Micro':<12}\")\n",
    "print(\"-\" * 70)\n",
    "for i in range(num_autoregressive_steps):\n",
    "    ar_temp = autoregressive_results[\"temp_mse\"][i]\n",
    "    ss_temp = single_step_temp_mse[i]\n",
    "    ar_micro = autoregressive_results[\"micro_mse\"][i]\n",
    "    ss_micro = single_step_micro_mse[i]\n",
    "    \n",
    "    delta_temp = ar_temp - ss_temp\n",
    "    delta_micro = ar_micro - ss_micro\n",
    "    \n",
    "    print(f\"{i+1:<8} {timesteps[i]:<10} {ar_temp:<12.6f} {ss_temp:<12.6f} {delta_temp:<+12.6f} {ar_micro:<12.6f} {ss_micro:<12.6f} {delta_micro:<+12.6f}\")\n",
    "\n",
    "print(\"-\" * 70)\n",
    "avg_temp_degradation = np.mean(np.array(autoregressive_results[\"temp_mse\"]) - np.array(single_step_temp_mse))\n",
    "avg_micro_degradation = np.mean(np.array(autoregressive_results[\"micro_mse\"]) - np.array(single_step_micro_mse))\n",
    "print(f\"{'Average degradation:':<30} {avg_temp_degradation:<+12.6f} {'':<12} {avg_micro_degradation:<+12.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.4 Evolution Visualization - Side-by-Side Comparison Over Time\n",
    "\n",
    "Visualize the temporal evolution of both ground truth and autoregressive predictions to see how well the model captures the dynamics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a comprehensive evolution visualization\n",
    "fig, axes = plt.subplots(2, num_autoregressive_steps + 1, figsize=(4 * (num_autoregressive_steps + 1), 8))\n",
    "\n",
    "# Get the initial microstructure state (last frame of input sequence)\n",
    "initial_micro_frame, _ = load_ground_truth_frame(temp_test_dataset, micro_test_dataset, selected_slice, sequence_length - 1)\n",
    "initial_micro_denorm = micro_test_dataset.denormalize(initial_micro_frame.unsqueeze(0))[0]\n",
    "initial_rgb = np.clip(np.transpose(initial_micro_denorm[0:3].numpy(), (2, 1, 0)), 0, 1).astype(np.float32)\n",
    "\n",
    "# First column: Initial state\n",
    "axes[0, 0].imshow(initial_rgb, aspect='equal', origin='lower')\n",
    "axes[0, 0].set_title(f'Initial State\\n(t={start_timestep + sequence_length - 1})')\n",
    "axes[0, 0].set_ylabel('Ground Truth\\nZ coordinate')\n",
    "\n",
    "axes[1, 0].imshow(initial_rgb, aspect='equal', origin='lower')\n",
    "axes[1, 0].set_title(f'Initial State\\n(t={start_timestep + sequence_length - 1})')\n",
    "axes[1, 0].set_ylabel('Prediction\\nZ coordinate')\n",
    "\n",
    "# Subsequent columns: Evolution over time\n",
    "for step in range(num_autoregressive_steps):\n",
    "    timestep = start_timestep + sequence_length + step\n",
    "    \n",
    "    # Ground truth\n",
    "    micro_target = autoregressive_results[\"micro_targets\"][step]\n",
    "    micro_target_denorm = micro_test_dataset.denormalize_target(micro_target)\n",
    "    target_rgb = np.clip(np.transpose(micro_target_denorm[0:3].numpy(), (2, 1, 0)), 0, 1).astype(np.float32)\n",
    "    \n",
    "    axes[0, step + 1].imshow(target_rgb, aspect='equal', origin='lower')\n",
    "    axes[0, step + 1].set_title(f't={timestep}')\n",
    "    \n",
    "    # Prediction\n",
    "    micro_pred = autoregressive_results[\"micro_predictions\"][step]\n",
    "    micro_pred_denorm = micro_test_dataset.denormalize_target(micro_pred)\n",
    "    pred_rgb = np.clip(np.transpose(micro_pred_denorm[0:3].numpy(), (2, 1, 0)), 0, 1).astype(np.float32)\n",
    "    \n",
    "    axes[1, step + 1].imshow(pred_rgb, aspect='equal', origin='lower')\n",
    "    mse = autoregressive_results[\"micro_mse\"][step]\n",
    "    axes[1, step + 1].set_title(f't={timestep}\\nMSE: {mse:.4f}')\n",
    "\n",
    "# Set x labels for bottom row\n",
    "for ax in axes[1, :]:\n",
    "    ax.set_xlabel('X coordinate')\n",
    "\n",
    "plt.suptitle(f'Microstructure Evolution: Ground Truth vs Autoregressive Prediction (Slice {selected_slice})', fontsize=14, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.savefig('microstructure_evolution_comparison.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.5 Summary and Conclusions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final summary of autoregressive cascaded prediction results\n",
    "print(\"=\" * 80)\n",
    "print(\"AUTOREGRESSIVE CASCADED PREDICTION - SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "print()\n",
    "print(f\"Configuration:\")\n",
    "print(f\"  - Selected slice: {selected_slice} (middle of domain)\")\n",
    "print(f\"  - Starting timestep: {start_timestep}\")\n",
    "print(f\"  - Input sequence length: {sequence_length}\")\n",
    "print(f\"  - Number of autoregressive steps: {num_autoregressive_steps}\")\n",
    "print(f\"  - Predicted timesteps: {[start_timestep + sequence_length + i for i in range(num_autoregressive_steps)]}\")\n",
    "print()\n",
    "print(\"-\" * 80)\n",
    "print(\"TEMPERATURE PREDICTION\")\n",
    "print(\"-\" * 80)\n",
    "print(f\"  Autoregressive:\")\n",
    "print(f\"    - Mean MSE: {np.mean(autoregressive_results['temp_mse']):.6f}\")\n",
    "print(f\"    - Final step MSE: {autoregressive_results['temp_mse'][-1]:.6f}\")\n",
    "print(f\"    - Error growth: {(autoregressive_results['temp_mse'][-1] / autoregressive_results['temp_mse'][0] - 1) * 100:.1f}%\")\n",
    "print(f\"  Single-step (baseline):\")\n",
    "print(f\"    - Mean MSE: {np.mean(single_step_temp_mse):.6f}\")\n",
    "print(f\"  Degradation from autoregressive:\")\n",
    "print(f\"    - Average: {avg_temp_degradation:+.6f} ({avg_temp_degradation / np.mean(single_step_temp_mse) * 100:+.1f}%)\")\n",
    "print()\n",
    "print(\"-\" * 80)\n",
    "print(\"MICROSTRUCTURE PREDICTION\")\n",
    "print(\"-\" * 80)\n",
    "print(f\"  Autoregressive:\")\n",
    "print(f\"    - Mean MSE: {np.mean(autoregressive_results['micro_mse']):.6f}\")\n",
    "print(f\"    - Final step MSE: {autoregressive_results['micro_mse'][-1]:.6f}\")\n",
    "print(f\"    - Error growth: {(autoregressive_results['micro_mse'][-1] / autoregressive_results['micro_mse'][0] - 1) * 100:.1f}%\")\n",
    "print(f\"  Single-step (baseline):\")\n",
    "print(f\"    - Mean MSE: {np.mean(single_step_micro_mse):.6f}\")\n",
    "print(f\"  Degradation from autoregressive:\")\n",
    "print(f\"    - Average: {avg_micro_degradation:+.6f} ({avg_micro_degradation / np.mean(single_step_micro_mse) * 100:+.1f}%)\")\n",
    "print()\n",
    "print(\"-\" * 80)\n",
    "print(\"KEY OBSERVATIONS\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# Analyze error trends\n",
    "temp_growth_rate = (autoregressive_results['temp_mse'][-1] / autoregressive_results['temp_mse'][0]) ** (1 / (num_autoregressive_steps - 1)) if num_autoregressive_steps > 1 else 1.0\n",
    "micro_growth_rate = (autoregressive_results['micro_mse'][-1] / autoregressive_results['micro_mse'][0]) ** (1 / (num_autoregressive_steps - 1)) if num_autoregressive_steps > 1 else 1.0\n",
    "\n",
    "print(f\"  1. Temperature error growth rate: {temp_growth_rate:.3f}x per step\")\n",
    "print(f\"  2. Microstructure error growth rate: {micro_growth_rate:.3f}x per step\")\n",
    "\n",
    "if avg_micro_degradation < 0.001:\n",
    "    print(f\"  3. Microstructure predictions are relatively stable despite using predicted temperature\")\n",
    "else:\n",
    "    print(f\"  3. Error accumulation causes noticeable degradation in microstructure predictions\")\n",
    "\n",
    "if temp_growth_rate < 1.5:\n",
    "    print(f\"  4. Temperature predictions remain relatively stable over {num_autoregressive_steps} steps\")\n",
    "else:\n",
    "    print(f\"  4. Temperature prediction error grows significantly over {num_autoregressive_steps} steps\")\n",
    "\n",
    "print()\n",
    "print(\"=\" * 80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lasernet",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
