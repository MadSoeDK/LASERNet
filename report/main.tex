\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts
% The preceding line is only needed to identify funding in the first footnote. If that is unneeded, please comment it out.
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage{bm}
\usepackage{amsmath}
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}

\DeclareMathOperator*{\argmin}{arg\,min}
%
\usepackage[T1]{fontenc}
% T1 fonts will be used to generate the final print and online PDFs,
% so please use T1 fonts in your manuscript whenever possible.
% Other font encondings may result in incorrect characters.
%
\usepackage{placeins}
\usepackage{graphicx}
% Used for displaying a sample figure. If possible, figure files should
% be included in EPS format.
%
% If you use the hyperref package, please uncomment the following two lines
% to display URLs in blue roman font according to Springer's eBook style:
\usepackage{color}
%\renewcommand\UrlFont{\color{blue}\rmfamily}

\usepackage{hyperref}
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage{float}
\usepackage{longtable}
\usepackage{import}
\usepackage{subcaption}
\usepackage{multirow}
\usepackage[sort&compress,square,comma,authoryear]{natbib}


% para poner comentarios dentro del texto...
% \usepackage{verbatim}
% \usepackage{xcolor}
% \usepackage{amsfonts}
% \usepackage{longtable}
% \usepackage{multirow}
%
\newcommand{\ojo}[1]{\textcolor{red}{\textbf{#1}}} 
\newcommand{\msb}[1]{\textcolor{blue}{#1}} 
%
\newcommand{\Prob}{\mathsf{P}}
\newcommand{\surface}{\mathbf{s}}
\newcommand{\paramData}{\mathcal{X}}
\newcommand{\outputData}{\mathcal{Y}}
\newcommand{\surfaceDomain}{\mathcal{S}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\Nd}{N_d}
\newcommand{\m}{\textbf{m}}
\newcommand{\Rd}{\R^d}
\newcommand{\Sym}{\mathrm{Sym}^+_d}
\newcommand{\Gd}{{G}_d}
\newcommand{\GMd}{{GM}_d}
\newcommand{\vmu}{\bm{\mu}}
\newcommand{\covar}{\bm{\Sigma}}
\newcommand{\wk}{w_k}
\newcommand{\vm}{\mathbf{m}}
\newcommand{\vx}{\mathbf{x}}
\newcommand{\etak}{{\eta_{k}}}
\newcommand{\probetak}{p_{\eta_{k}}}
\newcommand{\tmp}{\mathcal{T}}
\newcommand{\windows}{{J}}
\newcommand{\pred}{{\mathcal{\hat{Y}}}}
\newcommand{\loss}{L}


\begin{document}
%

% paper title
%\title{Fast unsupervised distribution shift detection system based on self-organizing clustering}

%\title{{Sequential Neural Modeling of Microstructure Evolution in Metal Additive Manufacturing}
\title{{Recurrent Neural Networks as Surrogate Models of Microstructure Evolution}
%in Metal Additive Manufacturing}

\thanks{This work was supported by ..... }}

%\title{Seasonal Extreme Maximal Temperature forecast \\ with the AutoGluonTS modern AutoML tool}


%
%\titlerunning{Abbreviated paper title}
% If the paper title is too long for the running head, you can set
% an abbreviated paper title here

% It should be annonymous for peer review!
\author{Anonymous}
%\author{
%\IEEEauthorblockN{
%Student A\IEEEauthorrefmark{1},
%Student B\IEEEauthorrefmark{1},
%Student C\IEEEauthorrefmark{1},
%Student D\IEEEauthorrefmark{1},
%Student E\IEEEauthorrefmark{1},
%Sebasti\'an Basterrech\IEEEauthorrefmark{1},
%Sankhya Mohanty\IEEEauthorrefmark{1}
%}
%\IEEEauthorblockA{\IEEEauthorrefmark{1}DTU-Construct, Technical University of Denmark, Kongens Lyngby, Denmark\\
%Emails: \texttt{sebbas@dtu.dk}, \texttt{samoah@dtu.dk}}
%}


\maketitle

\begin{abstract}
To be done at the end.
%
\end{abstract}

%  keywords


% For peerreview papers, this IEEEtran command inserts a page break and
% creates the second title. It will be ignored for other modes.
\IEEEpeerreviewmaketitle

\section*{To be done and general comments}
\begin{itemize}
    \item Weak points: bibliography should include much more papers.
    \item Weak points: problem specification, clear motivation, why to use predRNN, etc.
    \item Weak points: paper should be relevant for the NN conference, so we should be able to make the focus in NNs and how they are useful for computational mechanics.
    \item We should add also a visualization of the methodology pipeline, a diagram. 
    \item I wrote \textit{to be done} in some parts, but obviously, all of us will check the text and can participate in all the parts. It is just to divide the tasks, and create an initial draft.
    \item It would be good if all the students are listed as co‑authors, so that it is fair to the other students who contributed to the code and the report. Please talk with the students that didn't answer yet to the invitation.
    \item Unify the terminology, e.g., melt-pool melt pool, etc.
    
\end{itemize}

%        %%%%%%%%%%%%
\section{Introduction}\label{s:intro}
%        %%%%%%%%%%%%
\msb{To be done: SB}\\

Motivations

Potential of recurrent architectures for modeling ....

Recurrent neural architectures have achieved significant success in learning problems where the data consist of dependent instances and the ordering factor is important, such as time-series forecasting, time-series classification, sequence prediction and generative modeling~\cite{Basterrech2023}.
%
When to the temporal learning perspective we add the spatial ....



The main research hypothesis of this study is ... .....

Our objective consists of ....

Two, three lines about experiments.





In summary, the main contributions and novelties of this article are two-fold: (i)~....., and (ii)~.......
%
These contributions are particularly significant to ....
%


The rest of this article is organized as follows.....





%        %%%%%%%%%%%%
\section{Preliminaries and problem definition}\label{s:preliminaries}
%        %%%%%%%%%%%%

\subsection{Specification of the problem}
%\msb{To be done: SB}\\
%Describe the effect of the meltpool in laser based additive manufacturing.
% Describe the Inverse Pole Figure (IPF) add citations.
\noindent \textbf{Characterization of microstructure evolution using IPF maps}. The scanning electron microscopes are powerful instruments used to obtain detailed images of material's surface. Instead of light, the technology is based in a beam of electrons to scan the surface of a material.
%
The Electron Backscatter Diffraction (EBSD) is a widely used method in material science for characterizing microstructure features of polycrystalline materials such as metal and ceramics.
%
The microstructure of a material is composed by individual grains, and the EBSD data describe several aspects of those grains, that include the crystallographic orientation, grain size, type of boundaries, etc. 
%
A crystallographic visualization technique to represent the crystal orientation of each grain in EBSD data is the Inverse Pole Figure (IPF).
%
IPF provides a color map where each pixel corresponds to a specific grain orientation, which has become a standard approach to represent the texture of the material, visualize grain orientation distributions, and analyze microstructural features\msb{CITE}.
%
We applied \msb{FEM–PF} simulations for laser-based AM to create IPF visualizations which reflect how the grains change their shape and size over time as the laser scan across the material surface. 
%
Let $\vx$ denote a vector with the 3D spatial location of each voxel, its temperature and the information derived from the IPF (the crystallographic directions).
%

\noindent \textbf{Reduction of spatial complexity.}
%
Since the most relevant changes in the surface occur due to temperature variations, which happen around the region where the laser is scanning, using the full 3D representation at each time~$t$ as input to the NN introduces a large amount of redundant information and is computationally challenging.
%
Therefore, we reduce the spatial complexity to 2D planar slices that are used as input to the network. 
%
In the simulations, the laser scans along the~$x$-direction (track direction), therefore, the most relevant variations of the melt pool region are contained in the cross-section $(x,z)$~\cite{Khairallah2016}. 
%
Contrary, the cross-section~$(y,z)$, obtained by  fixing the coordinate along  the $x$-axis, shows weaker thermal gradient and less variation in the melt-pool dynamics, then it provides limited additional information across time.
%s

\noindent\textbf{Mathematical formulation.}
%\msb{To be done: SB}\\
We start by fixing the additional notation. Each 2D planar slice has dimension~$D\times H$, where~$D$ denotes the depth of the scanning along the track direction ($x$-axis) and $H$ denotes the height along the~$z$-axis.
%
In addition, let~$F$ be the number of feature channels including the IPF features (the crystallographic directions) and other simulated variables such as temperature.
%
Regarding the spatial structure, a tensor in ~$\R^{D\times H \times F}$  
represents the spatial information of each data point in the 2D planar slice.
%
Let's consider time~$t$ indexed over a discrete set~$\tmp$ (e.g., a segment of integers) in our notation.
%
Suppose we are capturing the dynamic behavior of the last~$\windows$ time units, which are used to make the temporal lag window that is given as input to the predictor.
%
The observations in a time window with~$T$ time units are collected in the sequence of tensors~$\mathcal{X}_{1:T}=\{\mathcal{X}_1,\ldots,\mathcal{X}_T\}$, where each tensor~$\mathcal{X}_i$ contains a time‑lag window of size $J$ and the spatial information. 
%
Then, each $\mathcal{X}_i$ with the spatial-temporal information lies in~$\R^{\windows \times D\times H \times F}$.
%

%forming a temporal lag window analogous to the sliding‑window inputs commonly used in time‑series prediction.
%
%With regard to the spatial structure, the last~$\windows$ measurements can be represented as a tensor~$\mathcal{X} \in \R^{\windows\times D\times H}$, where~$\mathcal{X}$ stacks~$\windows$ consecutive 2D slices across time.

The objective in a one-step ahead spatial-temporal modeling problem is to predict a future data point $\mathcal{X}_{T+1}$ given an input sequence~$\mathcal{X}_{1:T}$.
%$\pred_{T+1}\in\R^{D\times H}$ 
%
In this paper, the model predictor is a \msb{neural network} $f(\cdot,\bm{\theta})$ parametrized by $\bm{\theta}$. 
%
As usual, we train the network over a set of $N$ training samples consisting of sliding-window pairs
$\{(\mathcal{X}_{i:i+T-1},\, \mathcal{X}_{i+T})\}_{i=1}^{N}$, and the goal is to find an optimal set of weights $\bm{\theta}^*$ such that 
%
\begin{equation}
\label{loss}
\bm{\theta}^*
= \argmin_{\bm{\theta}}
      \frac{1}{N}\sum_{i=1}^{N}
      L\!\left(
          f(\mathcal{X}_{i:i+T-1}, \bm{\theta}),
          \,\mathcal{X}_{i+T}
      \right),
\end{equation}
where $L(\cdot,\cdot)$ is the loss function that measures the distance between the target and the prediction.
%
\subsection{Spatio-temporal prediction of image sequences using neural networks}
%\msb{To be done: SB, Description of ConvLSTM and PredRNNs}
%
%\noindent\textbf{Convolutional Neural Networks (CNNs).}
Most probably, the \msb{Convolutional Neural Networks (CNNs)} are the most commonly used architecture among deep learning models for classifying spatial data such as images, videos, and signals\cite{Schmidhuber2015, MORECITES}.
%
%
The core operation in CNNs is the convolution, which enables the extraction of local spatial features while exploiting parameter sharing to limit model complexity. As a result, CNNs are particularly suitable for high-dimensional imaging data where information is often encoded in local spatial structures~\cite{LeCun1998}.
%
On the other hand, \msb{LSTM} networks belong to the class of recurrent neural networks and are specifically designed to model temporal dependencies in sequential data~\cite{Schmidhuber2015}. Their internal gating mechanisms allow information to be selectively retained or discarded over time, enabling the representation of temporal correlations. This structure effectively mitigates the vanishing gradient problem, enabling the network to retain temporal information~\cite{Hochreiter1997}.
%


\subsection{Prediction in computational mechanics via neural networks} 
\msb{To be done: students and SB}\\

Literature review of similar problems in the last years.
We should improve the bibtex , we need more references \msb{To be done: students}\\


 
\section{Materials and methods}\label{sec:matandmet}
%
\subsection{Creation of a high-quality synthetic data for microstructures ....}\label{sec:dataset}


How data was created\\
\msb{To be done: students and Sankhya}\\


Characteristics of the data, format length, etc....
\msb{To be done: students}\\
From the raw data, there are $T_{\text{raw}} = 25$ timesteps, indexed by $t \in \{0, \ldots, 24\}$. The first timestep ($t=0$) is discarded, as it contains no usable information. The remaining sequence therefore consists of $T = 24$ timesteps, reindexed by $t \in \{0, \ldots, 23\}$. The dimensions of the coordinate system are $D=465$ ($x$-axis), $H=47$ ($z$-axis), and $W=94$ ($y$-axis). Each planar slice of the perpendicular axis($y$-axis) of the cross section ($x$,$z$) is assumed to be an independent sample. Let $N_{\perp}$ denote the number of samples obtained from the perpendicular $y$-axis. Further, we test temporal windows of sizes $L=2$ or $L=3$. The number of independent samples obtained by sliding the temporal window over $T$ timesteps with fixed planar slice is given by $N_L=W(T-L+1)$. Unless otherwise stated, a temporal window length of $L = 3$ is assumed throughout the remainder of this report giving $N_L=20$. The total number of independent samples, for each slice for each temporal window, is then given by $N=N_L \cdot N_{\perp}=20 \times 94=1880$. The number of feature channels for each sample depends on either temperature $F=1$ or microstructure $F=10$. 

Each input sample is represented as a tensor in 
\(
\mathbb{R}^{L \times D \times H \times F}
= \mathbb{R}^{3 \times 465 \times 47 \times F}.
\)
During training, inputs are grouped into batches of size $B = 16$.

Initial experiments using ConvLSTM or PredRNN directly on the raw input tensors resulted in flat training curves, indicating that the models failed to learn meaningful representations. One working hypothesis is the high dimensionality of the input data,
\(
L \cdot D \cdot H \cdot F = 3 \times 465 \times 47 \times 10 = 655,650
\),
makes optimization difficult and prevents the recurrent modules from learning useful spatiotemporal features when applied directly to the raw data. The encoder reduces the spatial dimensionality and projects the input into a lower-dimensional latent representation, which significantly improves trainability and leads to non-flat learning curves during training. The encoder, recurrent module (ConvLSTM or PredRNN), and decoder are optimized jointly within a single end-to-end architecture using backpropagation.

\subsection{Experimental settings}
\msb{To be done: students and SB}\\
Important hyperparameters
\begin{itemize}
    \item Dataset split(50,25,25)
    \item Temperature values for mask solidification front
    \item Sequence length
    \item Choice of loss function
    \item 
\end{itemize}

Regarding the evaluated architectures, including hyperparameters. Cite papers justifying the selection of the range of tested hyperparameters.

\subsection{Learning process}
\msb{To be done: students and SB}\\

Description of the evaluated algorithms and how the learning was done.


\msb{Here describe the temporal reduction, i.e., the input is reduced to a temporal window of length $L$.}


\subsection{Computational details.} 
\msb{To be done: students}\\



\section{Experimental results}\label{sec:results}
\msb{To be done: students and SB}\\

%

Should be both table and figures.

\section{Discussion}\label{sec:discussion}
\msb{To be done later}\\

%
It can be also a subsection of the analysis of results.



\section{Conclusions and future work}\label{sec:conclusions}
\msb{To be done later}\\

In this paper, we address the problem of....

~\\
The results show ..... 
~\\

Our investigation open up.... 
~\\

We plan to continue our research in ....
 
\bibliographystyle{IEEEtran}
\bibliography{References}
%
\end{document}
